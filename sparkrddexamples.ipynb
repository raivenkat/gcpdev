{"cells": [{"cell_type": "code", "execution_count": 10, "id": "9d1d457c-eb17-4a4f-9404-97bb8b3bf2d3", "metadata": {}, "outputs": [], "source": "# create spark session object\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('rdd_practicals').getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "5031c37b-a733-49b7-be3d-1fbb367f0fde", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "[1, 2, 3, 4, 5, 6, 10]"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "#create rdd from collection\n\ndata = [1,2,3,4,5,6,10]\nrdd = spark.sparkContext.parallelize(data)\nrdd.collect()\n"}, {"cell_type": "code", "execution_count": 6, "id": "c7c7231b-02a8-410d-9efa-b76c45f01d50", "metadata": {}, "outputs": [], "source": "# create rdd from a external file\n\ndata_path = \"gs://dev-de-training-default/vrai/test/wordcount.txt\"\n\nrdd = spark.sparkContext.textFile(data_path)\n#rdd.collect()"}, {"cell_type": "code", "execution_count": 8, "id": "9ec45938-226d-41a4-95ca-37bc288c5fea", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "[2, 4, 6, 8]"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "# Transformations in spark\n\n#==================>>>>>>>>>>>>>>>>>>> Filter\n\na = spark.sparkContext.parallelize(range(1,10))\n\nb = a.filter(lambda x: x % 2 == 0) \n\nb.collect()\n"}, {"cell_type": "code", "execution_count": 8, "id": "16c5caca-0fc6-4113-a182-bad059c58ab9", "metadata": {}, "outputs": [{"data": {"text/plain": "[[1, 9], [1, 16], [1, 25]]"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "#================>>>>>>>>>>>>>>> Map and FlatMap \n\nrdd1 = spark.sparkContext.parallelize([3,4,5]).map(lambda x: [1,x])\nrdd1.collect()\n"}, {"cell_type": "code", "execution_count": 24, "id": "8fd8908f-23ba-4dd3-9221-abd4aa9eb862", "metadata": {}, "outputs": [{"data": {"text/plain": "[1, 3, 1, 4, 1, 5]"}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": "rdd = spark.sparkContext.parallelize([3,4,5]).flatMap(lambda x: [1,x])\nrdd.collect()"}, {"cell_type": "code", "execution_count": 16, "id": "bb15e6a9-fc5d-4c2f-b218-055ddd86a576", "metadata": {}, "outputs": [{"data": {"text/plain": "[[3, 9], [4, 16], [5, 25]]"}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": "maprdd = spark.sparkContext.parallelize([3,4,5]).map(lambda x: [x,x*x])\nmaprdd.collect()"}, {"cell_type": "code", "execution_count": 15, "id": "5f868364-e77c-4d00-bbce-b8c26c699668", "metadata": {}, "outputs": [{"data": {"text/plain": "[3, 9, 4, 16, 5, 25]"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "flatmaprdd = spark.sparkContext.parallelize([3,4,5]).flatMap(lambda x: [x,x*x])\nflatmaprdd.collect()"}, {"cell_type": "code", "execution_count": 17, "id": "a07be0ef-961b-463c-a93e-7e1cd99921c3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "['Cat', 'Rat', 'Gnu', 'Dog']"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "\n#====================>>>>>>>>> distinct\n\ndistinct_rdd = spark.sparkContext.parallelize([\"Gnu\", \"Cat\", \"Rat\", \"Dog\", \"Gnu\", \"Rat\"],2) \ndistinct_rdd.distinct().collect()"}, {"cell_type": "code", "execution_count": 19, "id": "1e0a533a-5ca7-452e-a351-8dd1186b336e", "metadata": {}, "outputs": [], "source": "#========================>>>>>>>> cartesian\n\nx = spark.sparkContext.parallelize([1,2,3,4,5]) \ny = spark.sparkContext.parallelize([6,7,8,9,10]) \n\nx.cartesian(y).collect()"}, {"cell_type": "code", "execution_count": 22, "id": "7df0c085-3491-425f-be0c-998bd2008eab", "metadata": {}, "outputs": [{"data": {"text/plain": "2"}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": "\n#========================>>>>>>>> coalesce\n\ny = spark.sparkContext.parallelize(range(1,10), 10)\ny.getNumPartitions()\nz = y.coalesce(2)\nz.getNumPartitions()"}, {"cell_type": "code", "execution_count": 24, "id": "0bb680df-8fdd-41de-9421-bf2b264176e3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[('b', [2, 4]), ('a', [1, 3])]\n"}], "source": "\n#========================>>>>>>>> groupByKey\n\ndata = [(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4)]\nrdd = spark.sparkContext.parallelize(data)\n\n# Group by key\ngrouped_rdd = rdd.groupByKey().mapValues(list)\nprint(grouped_rdd.collect())  # Output: [('a', [1, 3]), ('b', [2, 4])]\n"}, {"cell_type": "code", "execution_count": null, "id": "cadedb3c-7e82-4437-8f85-1a1f44de6791", "metadata": {}, "outputs": [], "source": "#========================>>>>>>>> reduceByKey\n\ndata = [(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4)]\nrdd = spark.sparkContext.parallelize(data)\n\n# Sum values by key\nreduced_rdd = rdd.reduceByKey(lambda x, y: x + y)\nprint(reduced_rdd.collect())  \n"}, {"cell_type": "code", "execution_count": 26, "id": "e5cb3119-50ed-4a1a-8ef0-ee37f0870a15", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[('b', (2, 4)), ('a', (1, 3))]\n"}], "source": "#========================>>>>>>>> join\n\nrdd1 = sc.parallelize([(\"a\", 1), (\"b\", 2)])\nrdd2 = sc.parallelize([(\"a\", 3), (\"b\", 4)])\n\n# Join RDDs by key\njoined_rdd = rdd1.join(rdd2)\nprint(joined_rdd.collect())  \n"}, {"cell_type": "code", "execution_count": null, "id": "3302d2e2-e2bf-4d5e-8117-8c4045e848f3", "metadata": {}, "outputs": [], "source": "a = spark.sparkContext.parallelize([\"dog\", \"tiger\", \"lion\", \"cat\", \"panther\", \"eagle\"], 2) \n\nb = a.map(lambda x : (len(x), x)) \n\nb.keys().collect()"}, {"cell_type": "code", "execution_count": null, "id": "579514ce-6884-40b3-bd28-afdf906bed78", "metadata": {}, "outputs": [], "source": "#======================> Actions\n\nfrom pyspark import SparkContext\n\nsc = SparkContext(\"local\", \"Collect Example\")\ndata = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(data)\n\n# Collect all elements of the RDD\ncollected_data = rdd.collect()\nprint(collected_data)  # Output: [1, 2, 3, 4, 5]\n\n"}, {"cell_type": "code", "execution_count": null, "id": "794baf05-ab33-4d5e-87d3-aac3c7d32328", "metadata": {}, "outputs": [], "source": "# Count the number of elements in the RDD\ncount = rdd.count()\nprint(count)  # Output: 5\n"}, {"cell_type": "code", "execution_count": null, "id": "9f26e585-9c6d-4dfa-a65a-f9466d3395d2", "metadata": {}, "outputs": [], "source": "# Get the first element of the RDD\nfirst_element = rdd.first()\nprint(first_element)  # Output: 1\n"}, {"cell_type": "code", "execution_count": null, "id": "374ed07c-23d6-4edd-89eb-276264d16de6", "metadata": {}, "outputs": [], "source": "# Take the first 3 elements of the RDD\ntaken_elements = rdd.take(3)\nprint(taken_elements)  # Output: [1, 2, 3]\n"}, {"cell_type": "code", "execution_count": null, "id": "527bfea2-08a6-45d3-bbbf-8995432b31fa", "metadata": {}, "outputs": [], "source": "# Take a sample of 3 elements with replacement\nsampled_elements = rdd.takeSample(withReplacement=True, num=3)\nprint(sampled_elements)  # Output: [e.g., 3, 1, 4] (randomly sampled)\n"}, {"cell_type": "code", "execution_count": null, "id": "69aa8f8f-5239-4953-89b1-683389c311d6", "metadata": {}, "outputs": [], "source": "# Sum all elements of the RDD\nsum_of_elements = rdd.reduce(lambda x, y: x + y)\nprint(sum_of_elements)  # Output: 15\n"}, {"cell_type": "code", "execution_count": null, "id": "3730609a-e73d-4cac-a8fc-eb42de1c9b8c", "metadata": {}, "outputs": [], "source": "# Sum all elements with an initial value of 0\nsum_with_fold = rdd.fold(0, lambda x, y: x + y)\nprint(sum_with_fold)  # Output: 15\n"}, {"cell_type": "code", "execution_count": null, "id": "1804cca9-d55c-45bc-9f14-5317f6314b13", "metadata": {}, "outputs": [], "source": "data = [(\"a\", 1), (\"b\", 2), (\"a\", 3)]\nrdd = sc.parallelize(data)\n\n# Count occurrences of each key\ncount_by_key = rdd.countByKey()\nprint(dict(count_by_key))  # Output: {'a': 2, 'b': 1}\n"}, {"cell_type": "code", "execution_count": null, "id": "b2fe291a-c51d-4fe8-857e-fabc9a82b91e", "metadata": {}, "outputs": [], "source": "# Print each element\nrdd.foreach(lambda x: print(x))\n"}, {"cell_type": "code", "execution_count": null, "id": "81073509-07a3-4ff7-ba6b-5904e96fb536", "metadata": {}, "outputs": [], "source": "# Save RDD as text file (this won't work in a local notebook without a proper file system)\nrdd.saveAsTextFile(\"/path/to/output\")\n"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}}, "nbformat": 4, "nbformat_minor": 5}