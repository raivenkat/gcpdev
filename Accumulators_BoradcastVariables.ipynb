{"cells": [{"cell_type": "code", "execution_count": 1, "id": "3075bbcd-c71c-445d-b118-3fdcde8accdb", "metadata": {}, "outputs": [{"data": {"text/plain": "[('spark.eventLog.enabled', 'true'),\n ('spark.dynamicAllocation.minExecutors', '1'),\n ('spark.eventLog.dir',\n  'gs://dataproc-temp-us-west1-423929572205-dq3ru5gd/aebf22d7-7e3e-417d-be10-34d127f8f21e/spark-job-history'),\n ('spark.sql.warehouse.dir', 'file:/spark-warehouse'),\n ('spark.sql.autoBroadcastJoinThreshold', '21m'),\n ('spark.yarn.historyServer.address', 'jobs-cluster-m:18080'),\n ('spark.yarn.am.memory', '640m'),\n ('spark.history.fs.logDirectory',\n  'gs://dataproc-temp-us-west1-423929572205-dq3ru5gd/aebf22d7-7e3e-417d-be10-34d127f8f21e/spark-job-history'),\n ('spark.driver.appUIAddress',\n  'http://jobs-cluster-m.us-west1-a.c.dev-de-training.internal:34521'),\n ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n  'jobs-cluster-m'),\n ('spark.executor.instances', '2'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.yarn.unmanagedAM.enabled', 'true'),\n ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n  'http://jobs-cluster-m:8088/proxy/application_1724085230561_0003'),\n ('spark.submit.deployMode', 'client'),\n ('spark.extraListeners',\n  'com.google.cloud.spark.performance.DataprocMetricsListener'),\n ('spark.ui.filters',\n  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n ('spark.sql.cbo.joinReorder.enabled', 'true'),\n ('spark.shuffle.service.enabled', 'true'),\n ('spark.metrics.namespace',\n  'app_name:${spark.app.name}.app_id:${spark.app.id}'),\n ('spark.driver.maxResultSize', '1024m'),\n ('spark.scheduler.mode', 'FAIR'),\n ('spark.sql.adaptive.enabled', 'true'),\n ('spark.yarn.jars', 'local:/usr/lib/spark/jars/*'),\n ('spark.scheduler.minRegisteredResourcesRatio', '0.0'),\n ('spark.executor.id', 'driver'),\n ('spark.hadoop.hive.execution.engine', 'mr'),\n ('spark.app.name', 'PySparkShell'),\n ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n ('spark.dynamicAllocation.maxExecutors', '10000'),\n ('spark.app.startTime', '1724089714139'),\n ('spark.executor.memory', '2893m'),\n ('spark.master', 'yarn'),\n ('spark.driver.memory', '2048m'),\n ('spark.ui.port', '0'),\n ('spark.executorEnv.PYTHONPATH',\n  '/usr/lib/spark/python/lib/py4j-0.10.9-src.zip:/usr/lib/spark/python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip'),\n ('spark.sql.catalogImplementation', 'hive'),\n ('spark.rpc.message.maxSize', '512'),\n ('spark.rdd.compress', 'True'),\n ('spark.app.id', 'application_1724085230561_0003'),\n ('spark.driver.port', '41065'),\n ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n ('spark.submit.pyFiles', ''),\n ('spark.dynamicAllocation.enabled', 'true'),\n ('spark.yarn.isPython', 'true'),\n ('spark.executor.cores', '1'),\n ('spark.ui.proxyBase', '/proxy/application_1724085230561_0003'),\n ('spark.ui.showConsoleProgress', 'true'),\n ('spark.driver.host', 'jobs-cluster-m.us-west1-a.c.dev-de-training.internal'),\n ('spark.sql.cbo.enabled', 'true')]"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('rdd_practicals').getOrCreate()\n\nspark.sparkContext.getConf().getAll()"}, {"cell_type": "code", "execution_count": 1, "id": "84703eb2-ded2-40ce-976d-6f143d20bcf2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 0:>                                                          (0 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "[('John', 25), ('Jane', 30), ('Jake', 35)]\n"}, {"name": "stderr", "output_type": "stream", "text": "24/08/24 02:18:23 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for broadcast_0_python !\n"}], "source": "from pyspark import SparkContext\n\n\n# Large RDD\ndata_rdd = sc.parallelize([(\"John\", 25), (\"Jane\", 30), (\"Jake\", 35), (\"Jill\", 40), (\"Joe\", 45)])\n\n# Small set of allowed names\nallowed_names = {\"John\", \"Jane\", \"Jake\"}\n\n# Broadcast the allowed names\nbroadcast_names = sc.broadcast(allowed_names)\n\n# Filter the RDD based on the broadcast variable\nfiltered_rdd = data_rdd.filter(lambda x: x[0] in broadcast_names.value)\nprint(filtered_rdd.collect())  # Output: [('John', 25), ('Jane', 30), ('Jake', 35)]\n"}, {"cell_type": "code", "execution_count": 2, "id": "9a4197a4-b961-403b-b553-70859a43aacd", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:>                                                          (0 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "[100, 200, 400]\nNumber of errors: 2\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Create an accumulator to count errors\nerror_accumulator = sc.accumulator(0)\n\n# Example RDD with some invalid data\ndata_rdd = sc.parallelize([\"100\", \"200\", \"three hundred\", \"400\", \"five hundred\"])\n\n# Function to convert strings to integers and count errors\ndef safe_convert(x):\n    try:\n        return int(x)\n    except ValueError:\n        error_accumulator.add(1)\n        return None\n\n# Convert the RDD and count errors\nconverted_rdd = data_rdd.map(safe_convert).filter(lambda x: x is not None)\nprint(converted_rdd.collect())  # Output: [100, 200, 400]\nprint(f\"Number of errors: {error_accumulator.value}\")  # Output: Number of errors: 2\n"}, {"cell_type": "code", "execution_count": null, "id": "a7854d70-2c86-421c-a97f-7f59bfcf87d5", "metadata": {}, "outputs": [], "source": "# Large RDD of sales transactions: (product_id, amount)\nsales_rdd = sc.parallelize([\n    (\"p1\", 100), (\"p2\", 150), (\"p3\", 200), \n    (\"p4\", 250), (\"p1\", 300), (\"p5\", 350), \n    (\"invalid\", 400)\n])\n\n# Small dictionary mapping product IDs to categories\nproduct_categories = {\"p1\": \"electronics\", \"p2\": \"electronics\", \"p3\": \"clothing\", \"p4\": \"clothing\", \"p5\": \"groceries\"}\n\n# Broadcast the product categories\nbroadcast_categories = sc.broadcast(product_categories)\n\n# Accumulator to count invalid transactions\ninvalid_transactions = sc.accumulator(0)\n\n# Function to map product IDs to categories and handle errors\ndef map_to_category(sale):\n    product_id, amount = sale\n    if product_id in broadcast_categories.value:\n        category = broadcast_categories.value[product_id]\n        return (category, amount)\n    else:\n        invalid_transactions.add(1)\n        return None\n\n# Aggregate sales by category\ncategory_sales_rdd = sales_rdd.map(map_to_category).filter(lambda x: x is not None).reduceByKey(lambda x, y: x + y)\nprint(category_sales_rdd.collect())  # Output: [('electronics', 550), ('clothing', 450), ('groceries', 350)]\nprint(f\"Number of invalid transactions: {invalid_transactions.value}\")  # Output: Number of invalid transactions: 1\n"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}}, "nbformat": 4, "nbformat_minor": 5}